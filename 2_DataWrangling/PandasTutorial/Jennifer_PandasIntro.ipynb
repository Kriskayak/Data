{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling in Pandas\n",
    "<img src='images/img_one.jpg' align=right>\n",
    "The Journal of Data Science defines \"data science\" as almost everything that has something to do with data. In a job, this translates to using data to have an impact on the organization by adding value. Most commonly it is to use and apply the data to solve complex business problems. One of the most common steps taken in data science work is data wrangling. The following is a concise guide on how to go about exploring, manipulating and reshaping data in python using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/\">pandas</a> library.\n",
    " \n",
    "We will explore a breast cancer data set (credits: <a href=\"https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)\">UCI</a>) and use pandas to clean, reshape, massage and give us a clean data set, all of this will help dramatically increase the quality of our data. Note: Data quality is KEY for optimal performance with machine learning algorithms.\n",
    "\n",
    "The following pandas functionalities will be covered:\n",
    "1. Data exploration — columns, unique values in a column, describe, duplicates\n",
    "2. Dealing with missing values — quantifying missing values per column, filling & dropping missing values\n",
    "3. Reshaping data — one hot encoding, pivot tables, joins, grouping and aggregating\n",
    "4. Filtering data\n",
    "5. Other — Making descriptive columns, element-wise conditional operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Let us begin by reading in our dataset (csv file) into pandas and displaying the column names along with their data types. Also take a moment to view the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns =None\n",
    "pd.options.display.max_rows =40\n",
    "filename = 'data/breast_cancer_data.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of by actually looking at your data set\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the size of our dataset?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over here we see the columns names and their data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data we have the following columns as described by the source — Patient ID: id number, Clump Thickness: 1–10, Uniformity of Cell Size: 1–10, Uniformity of Cell Shape: 1–10, Marginal Adhesion: 1–10, Single Epithelial Cell Size: 1–10, Bare Nuclei: 1–10, Bland Chromatin: 1–10, Normal Nucleoli: 1–10, Mitoses: 1–10, Class: malignant or benign, Doctor name: 4 different doctors.\n",
    "\n",
    "Based on this, we can assume that patient_id is a unique identifier, class is going to tell us whether the tumor is malignant (cancerous) or benign (not cancerous). The remaining columns are numeric medical descriptions of the tumor, except for the doctor_name which is a categorical feature.\n",
    "\n",
    "Things to keep in mind — If our goal is to predict wether a tumor is cancerous or not based on the remaining features, we will have to one hot encode the categorical data and clean up the numerical data.\n",
    "From our first output we see that bare_nuclei was read as an object data type although the description is numeric. Therefore we will need to change this.\n",
    "\n",
    "To verify that our data matches up with the source we can use the describe option in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This provides some statistics on the numerical data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neatly summarizes some statistical data for all numerical columns. For categorical data we can hand this by grouping together values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This aggreates the data by its column names, then we pass the aggregation function (size = count)\n",
    "df.groupby(by =['class', 'doctor_name']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "\n",
    "With every dataset it is vital to evaluate the missing values. How many are there? Is it an error? Are there too many missing values? Does a missing value have a meaning relative to its context?\n",
    "We can sum up the total missing values using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with missing values? How many np.nan per column?\n",
    "\n",
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified our missing values, we have a few options. We can fill them in with a certain value (zero, mean/max/median by column, string) or drop them by row. Since there are few missing values, we can drop the rows to avoid skewing the data in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill with zero\n",
    "# df = df.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 0, how = 'any')  #drop rows with any column having np.nan values\n",
    "\n",
    "#Rename columns\n",
    "#df.rename(index =str, columns = {'patient_id':'patient_id'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to drop rows with any missing values in them.\n",
    "\n",
    "## Inspecting duplicates\n",
    "To view repeating rows we can start off by looking at the number of unique values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we list all columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Its good to inspect your unique key identifier\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that although there are 690 rows, there are only 637 unique patient_id’s. This could mean that some patient appear more than once in the dataset. To isolate these patients and view their data, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows rows that show up more than once and have the exact same column values. \n",
    "df[df.duplicated(keep = 'last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This shows all instances where patient_id shows up more than once, but may have varying column values\n",
    "df[df.duplicated(subset = 'patient_id', keep =False)].sort_values('patient_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that I have seen that there are some duplicates, I am going to go ahead and remove any duplicate rows\n",
    "\n",
    "#df = df.drop_duplicates(subset = None, keep ='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_patients = df.groupby(by = 'patient_id').size().sort_values(ascending =False)\n",
    "repeat_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that one patient shows up in the data 6 times!\n",
    "\n",
    "## Filtering data\n",
    "If we want to remove patients that show up more that 2 times in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_patients = repeat_patients[repeat_patients > 2].to_frame().reset_index()\n",
    "filtered_df = df[~df.patient_id.isin(filtered_patients.patient_id)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did not have the tilde (“~”) we would get all individuals that repeat more than twice. By adding a tilde the pandas boolean series is reversed and thus the resulting data frame is of those that do NOT repeat more than twice.\n",
    "\n",
    "## Reshaping data\n",
    "The dataset has elements of categorical data in the “doctor_name” column. To feed this data into a machine learning pipeline, we will need to convert it into a one hot encoded column. This can be done with a sci-kit learn package, however we will do it in pandas to demonstrate the pivoting and merging functionality. Start off by creating a new dataframe with the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = df[['patient_id','doctor_name']]\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This specifies all rows (':') and column name 'doctor_count'\n",
    "categorical_df.loc[:,'doctor_count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a column an extra column to identify which doctor a patient deals with. Pivot this table so that we only have numerical values in the cells and the columns become the doctors’ name. Then fill in the empty cells with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctors_one_hot_encoded = pd.pivot_table( categorical_df,\n",
    "                                  index = categorical_df.index, \n",
    "                                  columns = ['doctor_name'], \n",
    "                                   values = ['doctor_count'] )\n",
    "doctors_one_hot_encoded = doctors_one_hot_encoded.fillna(0)\n",
    "doctors_one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then drop the multiIndex columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctors_one_hot_encoded.columns = doctors_one_hot_encoded.columns.droplevel()\n",
    "doctors_one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now join this back to our main table. Typically a left join in pandas looks like this:\n",
    "\n",
    "`leftJoin_df = pd.merge(df1, df2, on ='col_name', how='left')`\n",
    "\n",
    "However we are joining on the index so we pass the “left_index” and “right_index” option to specify that the join key is the index of both tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(df, doctors_one_hot_encoded, left_index = True,right_index =True, how ='left')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the column that we no longer need by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['doctor_name'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row-wise Operations\n",
    "Another key component in data wrangling is having the ability to conduct row-wise or column wise operations. Examples of this are; rename elements within a column based on its value and create a new column that yields a specific value based on multiple attributes within the row.\n",
    "\n",
    "For this example lets create a new column that categorizes a patients cell as normal or abnormal based on its attributes. We first define our function and the operation that it will be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celltypelabel(x):\n",
    "    if ((x['cell_size_uniformity'] < 5) &      (x['cell_shape_uniformity'] < 5)):\n",
    "        \n",
    "        return('normal')\n",
    "    else:\n",
    "        return('abnormal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the pandas apply function to run the celltypelabel(x) function on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['cell_type_label'] = combined_df.apply(lambda x: celltypelabel(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Although some of these data manipulation steps can be done in SAS and excel. Doing it in python not only allows you to connect the data to vast open source resources in computer vision, machine and deep learning, but also for ETL automation purposes and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "1. Hot encode a new column in this dataset for cancerous (1) or not cancerous (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
